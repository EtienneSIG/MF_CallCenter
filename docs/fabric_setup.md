# Guide de DÃ©ploiement - Microsoft Fabric

## ğŸ¯ Objectif

Ce guide dÃ©crit **Ã©tape par Ã©tape** comment dÃ©ployer la dÃ©mo Customer 360 + Call Center dans Microsoft Fabric.

**PrÃ©requis** :
- Un compte Microsoft Fabric (trial ou licence)
- Les donnÃ©es gÃ©nÃ©rÃ©es localement (voir README.md)
- Un workspace Fabric crÃ©Ã©

**DurÃ©e estimÃ©e** : 30-45 minutes

---

## ğŸ“‹ Vue d'Ensemble du DÃ©ploiement

```
Ã‰tape 1: CrÃ©er un Lakehouse
Ã‰tape 2: Uploader les donnÃ©es vers OneLake
Ã‰tape 3: CrÃ©er des OneLake Shortcuts
Ã‰tape 4: Appliquer Shortcut Transformations AI sur les transcripts
Ã‰tape 5: Charger les CSV en tables Delta
Ã‰tape 6: CrÃ©er un Semantic Model
Ã‰tape 7: Configurer le Fabric Data Agent
Ã‰tape 8: Tester et valider
```

---

## Ã‰tape 1 : CrÃ©er un Lakehouse

### 1.1 AccÃ©der au Workspace

1. Ouvrir [Microsoft Fabric](https://app.fabric.microsoft.com/)
2. SÃ©lectionner ou crÃ©er un workspace (ex: `Demo-Customer360`)
3. VÃ©rifier que vous Ãªtes dans l'expÃ©rience **Data Engineering**

### 1.2 CrÃ©er le Lakehouse

1. Cliquer sur **+ New** â†’ **Lakehouse**
2. Nom : `Customer360_Lakehouse`
3. Cliquer sur **Create**

âœ… **RÃ©sultat attendu** : Un Lakehouse vide avec deux sections : **Tables** et **Files**.

---

## Ã‰tape 2 : Uploader les DonnÃ©es vers OneLake

### 2.1 PrÃ©parer les DonnÃ©es Locales

Sur votre machine locale, les donnÃ©es gÃ©nÃ©rÃ©es sont dans :
```
data/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ commerce/
â”‚   â”‚   â”œâ”€â”€ customers.csv
â”‚   â”‚   â”œâ”€â”€ products.csv
â”‚   â”‚   â”œâ”€â”€ orders.csv
â”‚   â”‚   â””â”€â”€ order_lines.csv
â”‚   â””â”€â”€ callcenter/
â”‚       â”œâ”€â”€ agents.csv
â”‚       â”œâ”€â”€ calls.csv
â”‚       â””â”€â”€ transcripts_txt/
â”‚           â”œâ”€â”€ CALL_000001.txt
â”‚           â”œâ”€â”€ CALL_000002.txt
â”‚           â””â”€â”€ ... (3000 fichiers)
```

### 2.2 Upload via l'Interface Fabric

**Option A : Upload direct (pour petits volumes)**

1. Dans le Lakehouse, aller dans **Files**
2. CrÃ©er une structure de dossiers :
   - Cliquer sur **Upload** â†’ **Upload folder**
   - SÃ©lectionner `data/raw/commerce`
   - RÃ©pÃ©ter pour `data/raw/callcenter`

**Option B : Upload via OneLake File Explorer (recommandÃ©)**

1. Installer [OneLake File Explorer](https://www.microsoft.com/en-us/download/details.aspx?id=105222) (Windows uniquement)
2. Ouvrir OneLake File Explorer
3. Naviguer vers votre workspace â†’ `Customer360_Lakehouse` â†’ **Files**
4. Copier-coller les dossiers `commerce/` et `callcenter/` depuis votre explorateur Windows

**Option C : Upload via API/CLI (pour automatisation)**

```bash
# NÃ©cessite azcopy ou un script Azure CLI
azcopy copy "data/raw/*" "https://<onelake-path>/Files/raw/" --recursive
```

âœ… **RÃ©sultat attendu** : Structure de dossiers visible dans **Files** du Lakehouse.

---

## Ã‰tape 3 : CrÃ©er des OneLake Shortcuts

### 3.1 Principe des Shortcuts

Les **OneLake Shortcuts** crÃ©ent des liens symboliques sans duplication de donnÃ©es.
Ils permettent de "monter" des donnÃ©es externes (ADLS, S3, etc.) ou internes (autre Lakehouse).

**Pour cette dÃ©mo** : On va crÃ©er des shortcuts vers les fichiers uploadÃ©s (optionnel si dÃ©jÃ  dans le Lakehouse, mais utile pour dÃ©montrer la fonctionnalitÃ©).

### 3.2 CrÃ©er un Shortcut (Exemple : CSV Commerce)

1. Dans le Lakehouse, section **Files**
2. Clic droit sur la racine â†’ **New shortcut**
3. Choisir **OneLake** (pour lier des fichiers dÃ©jÃ  dans Fabric)
4. SÃ©lectionner :
   - **Workspace** : Demo-Customer360
   - **Item** : Customer360_Lakehouse
   - **Path** : `Files/raw/commerce`
5. Nommer le shortcut : `commerce_data`
6. Cliquer sur **Create**

RÃ©pÃ©ter pour `callcenter` si vous voulez dÃ©montrer plusieurs shortcuts.

> **Note** : Si les fichiers sont dÃ©jÃ  dans le Lakehouse, cette Ã©tape est conceptuelle pour la dÃ©mo. 
> Dans un scÃ©nario rÃ©el, les shortcuts pointeraient vers un storage externe (ADLS Gen2, S3, etc.).

âœ… **RÃ©sultat attendu** : IcÃ´ne de shortcut visible dans Files, sans duplication de donnÃ©es.

---

## Ã‰tape 4 : Appliquer Shortcut Transformations AI sur les Transcripts

### 4.1 Principe des Shortcut Transformations

**Shortcut Transformations AI** (preview) transforme automatiquement des fichiers non structurÃ©s (txt, pdf, images) en tables Delta queryables.

Pour les transcripts `.txt`, Fabric peut extraire :
- **Sentiment** (positif/neutre/nÃ©gatif)
- **RÃ©sumÃ©** (summary du contenu)
- **PII Detection** (emails, tÃ©lÃ©phones, noms)
- **Topics** (sujets dÃ©tectÃ©s)

### 4.2 CrÃ©er une Transformation AI

1. Dans le Lakehouse, aller dans **Files** â†’ `raw/callcenter/transcripts_txt/`
2. Clic droit sur le dossier `transcripts_txt` â†’ **New AI transformation** (ou **Apply AI skills**)
   - Si l'option n'est pas visible, vÃ©rifier que la preview est activÃ©e dans les paramÃ¨tres du tenant
3. Configurer la transformation :
   - **Source** : `transcripts_txt/` (tous les .txt)
   - **Destination** : Table Delta `transcripts_transformed`
   - **AI Skills Ã  appliquer** :
     - âœ… Sentiment Analysis
     - âœ… Summarization
     - âœ… PII Detection
     - âœ… Key Phrase Extraction
4. Cliquer sur **Create transformation**

### 4.3 ExÃ©cuter la Transformation

1. La transformation se lance automatiquement
2. Suivre le progrÃ¨s dans le **Monitoring** (Activity pane)
3. Temps estimÃ© : 5-10 minutes pour 3000 fichiers (varie selon la charge Fabric)

âœ… **RÃ©sultat attendu** : Une nouvelle table Delta `transcripts_transformed` apparaÃ®t dans **Tables**.

### 4.4 VÃ©rifier le SchÃ©ma de la Table TransformÃ©e

Colonnes attendues :
- `call_id` (extrait du nom de fichier ou du contenu)
- `content` (texte complet)
- `summary` (rÃ©sumÃ© gÃ©nÃ©rÃ©)
- `sentiment` (positive/neutral/negative)
- `sentiment_score` (0-1)
- `pii_detected` (liste des PII trouvÃ©es)
- `key_phrases` (sujets principaux)
- `_metadata` (informations systÃ¨me)

**Exemple de requÃªte test** :
```sql
SELECT call_id, sentiment, summary, pii_detected
FROM transcripts_transformed
LIMIT 10;
```

> **Troubleshooting** : Si la table n'apparaÃ®t pas, rafraÃ®chir le Lakehouse ou vÃ©rifier les logs de transformation.

---

## Ã‰tape 5 : Charger les CSV en Tables Delta

### 5.1 CrÃ©er des Tables depuis les CSV

Pour chaque fichier CSV (`customers.csv`, `products.csv`, etc.), crÃ©er une table Delta.

**MÃ©thode A : Via l'interface (pour dÃ©mo interactive)**

1. Dans **Files**, naviguer vers `raw/commerce/customers.csv`
2. Clic droit â†’ **Load to new table**
3. Configurer :
   - **Table name** : `customers`
   - **Delimiter** : Comma
   - **First row has headers** : âœ… Yes
   - **Infer schema** : âœ… Yes
4. Cliquer sur **Load**

RÃ©pÃ©ter pour :
- `products` (products.csv)
- `orders` (orders.csv)
- `order_lines` (order_lines.csv)
- `agents` (agents.csv)
- `calls` (calls.csv)

**MÃ©thode B : Via Notebook (pour automatisation)**

CrÃ©er un Notebook dans le Lakehouse :

```python
# Notebook: Load CSV to Delta Tables

from pyspark.sql import SparkSession

# Chemins des fichiers
files = {
    "customers": "Files/raw/commerce/customers.csv",
    "products": "Files/raw/commerce/products.csv",
    "orders": "Files/raw/commerce/orders.csv",
    "order_lines": "Files/raw/commerce/order_lines.csv",
    "agents": "Files/raw/callcenter/agents.csv",
    "calls": "Files/raw/callcenter/calls.csv"
}

# Charger chaque CSV en table Delta
for table_name, file_path in files.items():
    df = spark.read.csv(file_path, header=True, inferSchema=True)
    df.write.format("delta").mode("overwrite").saveAsTable(table_name)
    print(f"âœ… Table {table_name} crÃ©Ã©e avec {df.count()} lignes")
```

ExÃ©cuter le notebook (Ctrl+Enter sur chaque cellule).

âœ… **RÃ©sultat attendu** : 6 tables Delta + 1 table `transcripts_transformed` = 7 tables au total dans **Tables**.

### 5.2 VÃ©rifier les Types de DonnÃ©es

Quelques vÃ©rifications importantes :

```sql
-- VÃ©rifier que les dates sont bien en TIMESTAMP
DESCRIBE customers;
-- Attendu: registration_date TIMESTAMP

DESCRIBE orders;
-- Attendu: order_date TIMESTAMP, delivery_date TIMESTAMP

-- VÃ©rifier les nombres
DESCRIBE order_lines;
-- Attendu: quantity INT, unit_price DECIMAL, total_price DECIMAL
```

Si les types sont incorrects (ex: date en STRING), ajuster avec :

```python
from pyspark.sql.functions import to_timestamp

df = spark.table("orders")
df = df.withColumn("order_date", to_timestamp("order_date", "yyyy-MM-dd HH:mm:ss"))
df.write.format("delta").mode("overwrite").saveAsTable("orders")
```

---

## Ã‰tape 6 : CrÃ©er un Semantic Model

Le **Semantic Model** (ex-Analysis Services) structure les donnÃ©es pour Power BI et le Data Agent.

### 6.1 CrÃ©er le Semantic Model

1. Dans le Lakehouse, cliquer sur **New semantic model** (en haut Ã  droite)
2. Nom : `Customer360_Model`
3. SÃ©lectionner les tables Ã  inclure :
   - âœ… customers
   - âœ… products
   - âœ… orders
   - âœ… order_lines
   - âœ… agents
   - âœ… calls
   - âœ… transcripts_transformed
4. Cliquer sur **Confirm**

### 6.2 DÃ©finir les Relations

Ouvrir le Semantic Model et crÃ©er les relations :

1. Cliquer sur **Model view** (icÃ´ne diagramme)
2. CrÃ©er les relations suivantes (drag & drop entre tables) :

| Table From | Colonne From | Table To | Colonne To | CardinalitÃ© |
|------------|--------------|----------|------------|-------------|
| `orders` | `customer_id` | `customers` | `customer_id` | Many-to-One |
| `order_lines` | `order_id` | `orders` | `order_id` | Many-to-One |
| `order_lines` | `product_id` | `products` | `product_id` | Many-to-One |
| `calls` | `customer_id` | `customers` | `customer_id` | Many-to-One |
| `calls` | `agent_id` | `agents` | `agent_id` | Many-to-One |
| `calls` | `order_id` | `orders` | `order_id` | Many-to-One (*) |
| `calls` | `product_id` | `products` | `product_id` | Many-to-One (*) |
| `calls` | `call_id` | `transcripts_transformed` | `call_id` | One-to-One |

(*) Ces relations sont "sparse" (beaucoup de NULLs). Configurer comme **Inactive** si nÃ©cessaire.

### 6.3 CrÃ©er des Mesures DAX

Dans le Semantic Model, aller dans **Data view** et crÃ©er une **New measure** :

```dax
// Mesures Commerce
Total Orders = COUNTROWS(orders)

Total Revenue = 
SUMX(
    order_lines,
    order_lines[quantity] * order_lines[unit_price] * (1 - order_lines[discount])
)

Avg Order Value = DIVIDE([Total Revenue], [Total Orders])

// Mesures Call Center
Total Calls = COUNTROWS(calls)

Avg Satisfaction = AVERAGE(calls[satisfaction])

Resolution Rate = 
DIVIDE(
    COUNTROWS(FILTER(calls, calls[resolved] = 1)),
    [Total Calls]
)

// Mesures combinÃ©es
Calls per Customer = 
DIVIDE(
    [Total Calls],
    DISTINCTCOUNT(calls[customer_id])
)

Callers Revenue = 
CALCULATE(
    [Total Revenue],
    FILTER(
        customers,
        COUNTROWS(RELATEDTABLE(calls)) > 0
    )
)
```

### 6.4 Publier le Semantic Model

1. Cliquer sur **File** â†’ **Save**
2. Le modÃ¨le est automatiquement publiÃ© dans le workspace

âœ… **RÃ©sultat attendu** : Semantic Model disponible dans le workspace, prÃªt pour Power BI et Data Agent.

---

## Ã‰tape 7 : Configurer le Fabric Data Agent

### 7.1 Activer la Preview Data Agent

1. Aller dans **Settings** (âš™ï¸) â†’ **Tenant settings** â†’ **Admin Portal**
2. Rechercher **Fabric Data Agent** (ou **Copilot for Data**)
3. Activer la preview pour le workspace

### 7.2 CrÃ©er le Data Agent

1. Dans le workspace, cliquer sur **+ New** â†’ **Data Agent** (ou **Copilot**)
2. Nom : `Customer360_Agent`
3. SÃ©lectionner la source :
   - **Type** : Semantic Model
   - **Source** : `Customer360_Model`
4. Cliquer sur **Create**

### 7.3 Configurer les Instructions (System Prompt)

1. Ouvrir le Data Agent
2. Aller dans **Settings** â†’ **Instructions**
3. Coller le contenu de [`data_agent_instructions.md`](data_agent_instructions.md) (voir Ã‰tape 8)
4. Sauvegarder

### 7.4 Tester le Data Agent

Poser une premiÃ¨re question :
```
Combien de clients avons-nous au total ?
```

RÃ©ponse attendue : `500 clients`

Si la rÃ©ponse est correcte âœ…, passer Ã  l'Ã©tape 8.

Si la rÃ©ponse est incorrecte âŒ :
- VÃ©rifier que le Semantic Model est bien publiÃ©
- VÃ©rifier les relations entre tables
- VÃ©rifier que les instructions sont bien configurÃ©es

---

## Ã‰tape 8 : Tester et Valider

### 8.1 Questions de Validation

Poser les questions de [`questions_demo.md`](questions_demo.md) :

1. âœ… Combien de clients avons-nous au total ?
2. âœ… Quelle est la rÃ©partition de nos clients par segment ?
3. âœ… Quel est le chiffre d'affaires total gÃ©nÃ©rÃ© ?
4. âœ… Quels sont les 5 produits les plus vendus ?
5. âœ… Quel est le taux de satisfaction moyen des appels ?

**CritÃ¨re de succÃ¨s** : Au moins 12/15 questions fonctionnent correctement.

### 8.2 CrÃ©er un Dashboard Power BI

1. Dans le workspace, cliquer sur **+ New** â†’ **Report**
2. SÃ©lectionner `Customer360_Model` comme source
3. CrÃ©er quelques visuels rapides :
   - Card : Total Customers, Total Orders, Total Revenue
   - Donut : Customers by Segment
   - Bar Chart : Top 5 Products
   - Line Chart : Revenue by Month
   - Gauge : Avg Satisfaction
4. Sauvegarder le rapport : `Customer360_Dashboard`

### 8.3 VÃ©rifier les Permissions

Si la dÃ©mo doit Ãªtre partagÃ©e :
1. Aller dans **Workspace settings** â†’ **Access**
2. Ajouter les viewers/contributors selon les besoins
3. VÃ©rifier que le Semantic Model est partagÃ© (hÃ©rite des permissions du workspace)

---

## ğŸ‰ DÃ©ploiement TerminÃ©

Vous avez maintenant :
- âœ… Un Lakehouse avec 7 tables Delta
- âœ… Des OneLake Shortcuts (optionnel)
- âœ… Des AI Transformations sur les transcripts
- âœ… Un Semantic Model avec relations et mesures
- âœ… Un Data Agent fonctionnel
- âœ… Un dashboard Power BI de base

**Prochaines Ã©tapes** :
- Tester les 15 questions de la dÃ©mo ([questions_demo.md](questions_demo.md))
- Personnaliser le dashboard Power BI
- PrÃ©parer le pitch de prÃ©sentation ([demo_story.md](demo_story.md))

---

## ğŸ”§ Troubleshooting

### ProblÃ¨me : Les transcripts ne sont pas transformÃ©s

**SymptÃ´mes** : La table `transcripts_transformed` n'existe pas

**Solutions** :
1. VÃ©rifier que la preview **Shortcut Transformations AI** est activÃ©e
2. VÃ©rifier que les fichiers .txt sont bien prÃ©sents dans `Files/raw/callcenter/transcripts_txt/`
3. RÃ©essayer la transformation manuellement
4. VÃ©rifier les quotas du tenant (limitations preview)

**Alternative** : CrÃ©er la table manuellement avec un Notebook :

```python
import os
from pyspark.sql.types import StructType, StructField, StringType

# Lire tous les fichiers .txt
transcripts = []
files_path = "/lakehouse/default/Files/raw/callcenter/transcripts_txt/"

for file in os.listdir(files_path):
    if file.endswith(".txt"):
        with open(os.path.join(files_path, file), "r", encoding="utf-8") as f:
            content = f.read()
            call_id = file.replace(".txt", "")
            # Parsing simple (Ã  amÃ©liorer)
            lines = content.split("\n")
            transcripts.append({
                "call_id": call_id,
                "content": content,
                "summary": "Manual summary",  # Ã€ gÃ©nÃ©rer avec Azure OpenAI si besoin
                "sentiment": "neutral"  # Ã€ calculer
            })

# CrÃ©er DataFrame et table Delta
schema = StructType([
    StructField("call_id", StringType(), False),
    StructField("content", StringType(), True),
    StructField("summary", StringType(), True),
    StructField("sentiment", StringType(), True)
])

df = spark.createDataFrame(transcripts, schema)
df.write.format("delta").mode("overwrite").saveAsTable("transcripts_transformed")
```

---

### ProblÃ¨me : Le Data Agent ne rÃ©pond pas correctement

**SymptÃ´mes** : RÃ©ponses incohÃ©rentes ou erreurs

**Solutions** :
1. VÃ©rifier que le Semantic Model est publiÃ© (statut "Active")
2. VÃ©rifier les relations entre tables (doivent Ãªtre correctes)
3. Simplifier la question (utiliser des termes exacts des colonnes)
4. Consulter les instructions du Data Agent et ajuster si nÃ©cessaire
5. VÃ©rifier les logs d'erreur dans **Monitoring**

**Exemple** :
- âŒ "Quel est le type de clients ?" (ambigu)
- âœ… "Quelle est la rÃ©partition par segment ?" (terme exact : `segment`)

---

### ProblÃ¨me : Erreurs de type de donnÃ©es

**SymptÃ´mes** : Les dates sont en texte, les calculs Ã©chouent

**Solutions** :
1. RÃ©importer les CSV avec `inferSchema=True` (Notebook)
2. Caster manuellement les colonnes :

```python
from pyspark.sql.functions import to_timestamp, col

df = spark.table("orders")
df = df.withColumn("order_date", to_timestamp(col("order_date"), "yyyy-MM-dd HH:mm:ss"))
df = df.withColumn("delivery_date", to_timestamp(col("delivery_date"), "yyyy-MM-dd HH:mm:ss"))
df.write.format("delta").mode("overwrite").option("overwriteSchema", "true").saveAsTable("orders")
```

3. VÃ©rifier l'encodage UTF-8 des CSV (pas de BOM)

---

### ProblÃ¨me : Permissions insuffisantes

**SymptÃ´mes** : "Access denied" ou "Not authorized"

**Solutions** :
1. VÃ©rifier que vous Ãªtes **Admin** ou **Member** du workspace
2. VÃ©rifier les permissions sur le Lakehouse (doit Ãªtre partagÃ©)
3. VÃ©rifier les permissions sur le Semantic Model (hÃ©rite du workspace par dÃ©faut)

---

## ğŸ“š Ressources ComplÃ©mentaires

- [Documentation OneLake Shortcuts](https://learn.microsoft.com/en-us/fabric/onelake/onelake-shortcuts)
- [AI Transformations in Fabric](https://learn.microsoft.com/en-us/fabric/data-engineering/ai-transformations)
- [Fabric Data Agent (Copilot)](https://learn.microsoft.com/en-us/fabric/data-science/data-agent)
- [Semantic Model Best Practices](https://learn.microsoft.com/en-us/power-bi/guidance/star-schema)

---

## âœ… Checklist de DÃ©ploiement

Cochez au fur et Ã  mesure :

- [ ] Lakehouse crÃ©Ã©
- [ ] DonnÃ©es uploadÃ©es (CSV + transcripts .txt)
- [ ] OneLake Shortcuts crÃ©Ã©s (optionnel)
- [ ] AI Transformations appliquÃ©es sur transcripts
- [ ] 7 tables Delta crÃ©Ã©es et vÃ©rifiÃ©es
- [ ] Semantic Model crÃ©Ã©
- [ ] Relations dÃ©finies dans le modÃ¨le
- [ ] Mesures DAX ajoutÃ©es
- [ ] Data Agent configurÃ©
- [ ] Instructions du Data Agent ajoutÃ©es
- [ ] Questions de test validÃ©es (â‰¥12/15)
- [ ] Dashboard Power BI crÃ©Ã©
- [ ] Permissions partagÃ©es (si nÃ©cessaire)

**Si toutes les cases sont cochÃ©es, la dÃ©mo est prÃªte ! ğŸš€**
